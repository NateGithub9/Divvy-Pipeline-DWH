{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f846ea8-4bd6-483f-8344-d0acbb46ebcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DIM DATE DAY TABLE\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, date_format, year, quarter, month, dayofmonth, dayofweek, \n",
    "    when, lit, expr\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Add columns\n",
    "dim_date_df = dates_df \\\n",
    "    .withColumn(\"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\").cast(\"bigint\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month_Name\", date_format(col(\"Full_Date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"Day_of_Month\", dayofmonth(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_of_Week\", dayofweek(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_Name\", date_format(col(\"Full_Date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"Is_Weekend\", when(col(\"Day_of_Week\").isin([1,7]), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\n",
    "        \"Season\",\n",
    "        when(month(col(\"Full_Date\")).isin([12,1,2]), \"Winter\")\n",
    "        .when(month(col(\"Full_Date\")).isin([3,4,5]), \"Spring\")\n",
    "        .when(month(col(\"Full_Date\")).isin([6,7,8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    )\n",
    "\n",
    "# Reorder columns\n",
    "dim_date_df = dim_date_df.select(\n",
    "    \"Date_Key\", \"Full_Date\", \"Year\", \"Quarter\", \"Month\", \"Month_Name\",\n",
    "    \"Day_of_Month\", \"Day_of_Week\", \"Day_Name\", \"Is_Weekend\", \"Season\"\n",
    ")\n",
    "\n",
    "# Create managed table with correct schema and primary key\n",
    "dim_date_df.write \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"divvy.default.DIM_Date_day\")\n",
    "display(dim_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8410f2-0b29-47e6-8bed-58043e84a106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--dim_date_day_pk\n",
    "ALTER TABLE divvy.default.dim_date_day\n",
    "ALTER COLUMN Date_Key SET NOT NULL;\n",
    "\n",
    "ALTER TABLE divvy.default.dim_date_day\n",
    "ADD CONSTRAINT dim_date_day_pk PRIMARY KEY (Date_Key)\n",
    "NOT ENFORCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9adfd646-f21c-4cd1-8a7a-7a0acaa4e190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DIM DATE HOUR TABLE\n",
    "\n",
    "from pyspark.sql.functions import explode, sequence, to_date, date_format, col, lit, when\n",
    "\n",
    "# Define date and hour range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Create hour and minute bucket DataFrame\n",
    "hours = spark.range(0, 24).withColumnRenamed(\"id\", \"Hour\")\n",
    "minute_buckets = spark.createDataFrame([(0,), (15,), (30,), (45,)], [\"MinuteBucket\"])\n",
    "\n",
    "# Cross join dates, hours, and minute buckets\n",
    "dim_date_hour_df = dates_df.crossJoin(hours).crossJoin(minute_buckets)\n",
    "\n",
    "# Add Date_Key\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\").cast(\"bigint\")\n",
    ")\n",
    "\n",
    "# IsRushHour: 7-9am and 16-18pm\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"IsRushHour\",\n",
    "    when(\n",
    "        ((col(\"Hour\").between(7, 9)) | (col(\"Hour\").between(16, 18))),\n",
    "        lit(True)\n",
    "    ).otherwise(lit(False))\n",
    ")\n",
    "\n",
    "# Timeofday: Night (0-5), Morning (6-11), Afternoon (12-17), Evening (18-23)\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"Timeofday\",\n",
    "    when(col(\"Hour\").between(0, 5), \"Night\")\n",
    "    .when(col(\"Hour\").between(6, 11), \"Morning\")\n",
    "    .when(col(\"Hour\").between(12, 17), \"Afternoon\")\n",
    "    .otherwise(\"Evening\")\n",
    ")\n",
    "\n",
    "# Select and reorder columns\n",
    "dim_date_hour_df = dim_date_hour_df.select(\n",
    "    \"Date_Key\", \"Hour\", \"MinuteBucket\", \"IsRushHour\", \"Timeofday\"\n",
    ")\n",
    "\n",
    "# Create managed table\n",
    "dim_date_hour_df.write.mode(\"overwrite\").saveAsTable(\"divvy.default.DIM_date_hour\")\n",
    "\n",
    "display(dim_date_hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c43f81-2b18-446a-ac1f-8ed7896672f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--dim_date_hour_pk\n",
    "ALTER TABLE dim_date_hour\n",
    "ALTER COLUMN Date_Key SET NOT NULL;\n",
    "\n",
    "ALTER TABLE dim_date_hour\n",
    "ADD CONSTRAINT dim_date_hour_pk PRIMARY KEY (Date_Key)\n",
    "NOT ENFORCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14ce24a-5d77-4441-a8fe-93a506d4d76f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sql\n",
    "-- DIM Station\n",
    "\n",
    "CREATE TABLE Dim_Station (\n",
    "  Station_Key BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  Station_Name STRING,\n",
    "  Latitude DOUBLE,\n",
    "  Longitude DOUBLE,\n",
    "  CONSTRAINT pk_Station_Key PRIMARY KEY (Station_Key)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b898c0f-a0de-4e2e-a499-05381d8f5e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE dim_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaad657c-9428-408e-8238-2ce8bb619ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DIM Station\n",
    "CREATE TABLE dim_station\n",
    "(\n",
    "    -- SURROGATE KEY (PK) - Auto-incrementing\n",
    "    Station_Key             BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    -- NATURAL KEY (for JOIN/MERGE)\n",
    "    Station_ID_Natural      STRING NOT NULL COMMENT 'The original station ID from the source data.',\n",
    "    -- 3. Descriptive Attributes (SCD Type 1 candidates)\n",
    "    Station_Name            STRING,\n",
    "    Latitude                DOUBLE,\n",
    "    Longitude               DOUBLE,\n",
    "    CONSTRAINT pk_station_key PRIMARY KEY (Station_Key)\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd7239e-d715-44ef-92bb-853bdfad7dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE Dim_Rider (\n",
    "  Rider_key BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  Bike_Type STRING,\n",
    "  CONSTRAINT pk_Rider_key PRIMARY KEY (Rider_key)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c355f46b-8af7-49f1-8eda-94a9c1d73665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DIM Weather\n",
    "\n",
    "CREATE OR REPLACE TABLE dim_weather (\n",
    "    Weather_Key TIMESTAMP, \n",
    "    Temp_type_text STRING,\n",
    "    Temp_type_bin DOUBLE,\n",
    "    Humidity_categories_text STRING,\n",
    "    Humidity_categories_bin DOUBLE,\n",
    "    Precipitation_types_text STRING,\n",
    "    Precipitation_types_bin DOUBLE,\n",
    "    Wind_type_categories_text STRING,\n",
    "    Wind_type_categories_bin DOUBLE,\n",
    "    PRIMARY KEY (Weather_Key) NOT ENFORCED\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6473133-3ea0-4569-aca0-2a3a332cae81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP table fact_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2e1e57-3172-45c8-8e58-5b1a68f7231b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FACT TRIP TABLE\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_trip (\n",
    "  Trip_Key STRING,\n",
    "  -- Foreign Keys\n",
    "  Start_Date_Key BIGINT,\n",
    "  Start_Date_Hour_Key BIGINT,\n",
    "  End_Date_Key BIGINT,\n",
    "  End_Date_Hour_Key BIGINT,\n",
    "  Rider_Key BIGINT,\n",
    "  Start_Station_Key BIGINT,\n",
    "  End_Station_Key BIGINT,\n",
    "  Weather_Key TIMESTAMP,\n",
    "  -- Measures\n",
    "  Trip_Duration_Minutes DOUBLE,\n",
    "  Trip_Distance_Km DOUBLE,\n",
    "\n",
    "  -- Primary Key\n",
    "  CONSTRAINT pk_fact_trip PRIMARY KEY (Trip_Key) NOT ENFORCED,\n",
    "\n",
    "  CONSTRAINT fk_start_date FOREIGN KEY (Start_Date_Key) REFERENCES dim_date_day(Date_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_start_hour FOREIGN KEY (Start_Date_Hour_Key) REFERENCES dim_date_hour(Date_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_end_date FOREIGN KEY (End_Date_Key) REFERENCES dim_date_day(Date_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_end_hour FOREIGN KEY (End_Date_Hour_Key) REFERENCES dim_date_hour(Date_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_rider FOREIGN KEY (Rider_Key) REFERENCES dim_rider(Rider_key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_start_station FOREIGN KEY (Start_Station_Key) REFERENCES dim_station(Station_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_end_station FOREIGN KEY (End_Station_Key) REFERENCES dim_station(Station_Key) NOT ENFORCED,\n",
    "  CONSTRAINT fk_weather FOREIGN KEY (Weather_Key) REFERENCES dim_weather(Weather_Key) NOT ENFORCED\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7542444513201540,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
