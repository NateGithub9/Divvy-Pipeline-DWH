{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5eb533-14f2-4aa5-8b07-16bc59372e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE bronze_weather_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b326d492-4614-42a8-8987-20f6060f71d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "ZIP_SOURCE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips\"\n",
    "UNZIPPED_DEST_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/raw\"\n",
    "ARCHIVE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips_archive\"\n",
    "\n",
    "dbutils.fs.mkdirs(ARCHIVE_PATH)\n",
    "\n",
    "def extract_and_write_zip_files():\n",
    "    zip_files = [\n",
    "        f.path for f in dbutils.fs.ls(ZIP_SOURCE_PATH)\n",
    "        if f.name.endswith('.zip')\n",
    "    ]\n",
    "    for zip_path in zip_files:\n",
    "        # Read ZIP file as binary\n",
    "        binary_df = spark.read.format(\"binaryFile\").load(zip_path)\n",
    "        zip_bytes = binary_df.collect()[0]['content']\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zip_ref:\n",
    "            for contained_file in zip_ref.namelist():\n",
    "                if contained_file.lower().endswith('.csv'):\n",
    "                    csv_bytes = zip_ref.read(contained_file)\n",
    "                    output_file_path = f\"{UNZIPPED_DEST_PATH}/{contained_file}\"\n",
    "                    dbutils.fs.put(\n",
    "                        output_file_path,\n",
    "                        csv_bytes.decode(\"utf-8\", errors='ignore'),\n",
    "                        overwrite=True\n",
    "                    )\n",
    "        # Move processed zip to archive\n",
    "        file_name = zip_path.split('/')[-1]\n",
    "        archive_target = f\"{ARCHIVE_PATH}/{file_name}\"\n",
    "        dbutils.fs.mv(zip_path, archive_target)\n",
    "\n",
    "extract_and_write_zip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6156b07-9400-4923-bdb4-7589ccb4f367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trip Data\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "trip_schema = StructType([\n",
    "  StructField(\"ride_id\", StringType(), True),\n",
    "  StructField(\"rideable_type\", StringType(), True),\n",
    "  StructField(\"started_at\", StringType(), True),\n",
    "  StructField(\"ended_at\", StringType(), True),\n",
    "  StructField(\"start_station_name\", StringType(), True),\n",
    "  StructField(\"start_station_id\", StringType(), True),\n",
    "  StructField(\"end_station_name\", StringType(), True),\n",
    "  StructField(\"end_station_id\", StringType(), True),\n",
    "  StructField(\"start_lat\", StringType(), True),\n",
    "  StructField(\"start_lng\", StringType(), True),\n",
    "  StructField(\"end_lat\", StringType(), True),\n",
    "  StructField(\"end_lng\", StringType(), True),\n",
    "  StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt//trips/trip_data/schema\")\n",
    "  .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")         \n",
    "  .schema(trip_schema)\n",
    "  .load(\"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips\") \n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", \"dbfs:/mnt/trips/trip_data/data\")\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(\"bronze_trip_data\")\n",
    ").awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72b5871-47fd-4827-b1c6-80b0df2813e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather Data\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "weather_schema = StructType([\n",
    "  StructField(\"YEAR\", StringType(), True),\n",
    "  StructField(\"MO\", StringType(), True),\n",
    "  StructField(\"DY\", StringType(), True),\n",
    "  StructField(\"HR\", StringType(), True),\n",
    "  StructField(\"TEMP\", StringType(), True),\n",
    "  StructField(\"PRCP\", StringType(), True),\n",
    "  StructField(\"HMDT\", StringType(), True),\n",
    "  StructField(\"WND_SPD\", StringType(), True),\n",
    "  StructField(\"ATM_PRESS\", StringType(), True),\n",
    "  StructField(\"REF\", StringType(), True)\n",
    "])\n",
    "\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"header\", \"true\") \n",
    "  .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt/weather/weather_data/schema\")\n",
    "  .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "  .schema(weather_schema)\n",
    "  .load(\"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips\")\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", \"dbfs:/mnt/weather_data/data\")\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(\"bronze_weather_data\")\n",
    ").awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b37b51-bea4-4fca-a4a8-3e93403278ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Trip Data (silver layer)\n",
    "CREATE OR REPLACE TABLE silver_ride_fact\n",
    "(\n",
    "  ride_key BIGINT GENERATED ALWAYS AS IDENTITY, -- Surrogate Key for the Fact Table\n",
    "  ride_id STRING,\n",
    "  rideable_type STRING,\n",
    "  started_at TIMESTAMP,\n",
    "  ended_at TIMESTAMP,\n",
    "  start_station_id STRING, -- Keep as STRING based on data sample\n",
    "  end_station_id STRING,   -- Keep as STRING based on data sample\n",
    "  start_lat DOUBLE,\n",
    "  start_lng DOUBLE,\n",
    "  end_lat DOUBLE,\n",
    "  end_lng DOUBLE,\n",
    "  member_casual STRING\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Curated fact table with correct data types and data quality checks';\n",
    "\n",
    "-- Insert (or use MERGE INTO for updates/deduplication) the transformed data\n",
    "INSERT INTO silver_ride_fact\n",
    "SELECT\n",
    "  ride_id,\n",
    "  rideable_type,\n",
    "  CAST(started_at AS TIMESTAMP) AS started_at,\n",
    "  CAST(ended_at AS TIMESTAMP) AS ended_at,\n",
    "  start_station_id,\n",
    "  end_station_id,\n",
    "  CAST(start_lat AS DOUBLE) AS start_lat,\n",
    "  CAST(start_lng AS DOUBLE) AS start_lng,\n",
    "  CAST(end_lat AS DOUBLE) AS end_lat,\n",
    "  CAST(end_lng AS DOUBLE) AS end_lng,\n",
    "  member_casual\n",
    "FROM\n",
    "  bronze_ride_data\n",
    "-- Optional: Add a WHERE clause here for basic data quality filtering (e.g., non-null ride_id)\n",
    "WHERE ride_id IS NOT NULL;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5345400243554991,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Draft",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
