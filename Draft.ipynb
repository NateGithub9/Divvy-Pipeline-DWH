{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b326d492-4614-42a8-8987-20f6060f71d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#unzip \n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "ZIP_SOURCE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips\"\n",
    "UNZIPPED_DEST_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/raw\"\n",
    "ARCHIVE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips_archive\"\n",
    "\n",
    "dbutils.fs.mkdirs(ARCHIVE_PATH)\n",
    "\n",
    "def extract_and_write_zip_files():\n",
    "    zip_files = [\n",
    "        f.path for f in dbutils.fs.ls(ZIP_SOURCE_PATH)\n",
    "        if f.name.endswith('.zip')\n",
    "    ]\n",
    "    for zip_path in zip_files:\n",
    "        # Read ZIP file as binary\n",
    "        binary_df = spark.read.format(\"binaryFile\").load(zip_path)\n",
    "        zip_bytes = binary_df.collect()[0]['content']\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zip_ref:\n",
    "            for contained_file in zip_ref.namelist():\n",
    "                if contained_file.lower().endswith('.csv'):\n",
    "                    csv_bytes = zip_ref.read(contained_file)\n",
    "                    output_file_path = f\"{UNZIPPED_DEST_PATH}/{contained_file}\"\n",
    "                    dbutils.fs.put(\n",
    "                        output_file_path,\n",
    "                        csv_bytes.decode(\"utf-8\", errors='ignore'),\n",
    "                        overwrite=True\n",
    "                    )\n",
    "        # Move processed zip to archive\n",
    "        file_name = zip_path.split('/')[-1]\n",
    "        archive_target = f\"{ARCHIVE_PATH}/{file_name}\"\n",
    "        dbutils.fs.mv(zip_path, archive_target)\n",
    "\n",
    "extract_and_write_zip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f43e1edb-a177-48d6-896a-d76d29ddcc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trip column type\n",
    "df = spark.table(\"divvy.default.bronze_trip_data\")\n",
    "display(spark.createDataFrame(df.dtypes, [\"column\", \"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "501df412-f205-4583-b443-edf73be76b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#weather column type\n",
    "df = spark.table(\"divvy.default.bronze_weather_data\")\n",
    "display(spark.createDataFrame(df.dtypes, [\"column\", \"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab62777c-5a16-420f-9118-8e74846a3ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, date_format, year, quarter, month, dayofmonth, dayofweek, \n",
    "    when, lit, expr\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Add columns\n",
    "dim_date_df = dates_df \\\n",
    "    .withColumn(\"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Year\", year(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month_Name\", date_format(col(\"Full_Date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"Day_of_Month\", dayofmonth(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_of_Week\", dayofweek(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_Name\", date_format(col(\"Full_Date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"Is_Weekend\", when(col(\"Day_of_Week\").isin([1,7]), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\n",
    "        \"Season\",\n",
    "        when(month(col(\"Full_Date\")).isin([12,1,2]), \"Winter\")\n",
    "        .when(month(col(\"Full_Date\")).isin([3,4,5]), \"Spring\")\n",
    "        .when(month(col(\"Full_Date\")).isin([6,7,8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    )\n",
    "\n",
    "# Reorder columns\n",
    "dim_date_df = dim_date_df.select(\n",
    "    \"Date_Key\", \"Full_Date\", \"Year\", \"Quarter\", \"Month\", \"Month_Name\",\n",
    "    \"Day_of_Month\", \"Day_of_Week\", \"Day_Name\", \"Is_Weekend\", \"Season\"\n",
    ")\n",
    "\n",
    "# Create managed table\n",
    "dim_date_df.write.mode(\"overwrite\").saveAsTable(\"divvy.default.DIM_Date_day\")\n",
    "\n",
    "display(dim_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dad4b9-a805-4cec-a9a2-349855c7283e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, sequence, to_date, date_format, col, lit, when\n",
    "\n",
    "# Define date and hour range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Create hour and minute bucket DataFrame\n",
    "hours = spark.range(0, 24).withColumnRenamed(\"id\", \"Hour\")\n",
    "minute_buckets = spark.createDataFrame([(0,), (15,), (30,), (45,)], [\"MinuteBucket\"])\n",
    "\n",
    "# Cross join dates, hours, and minute buckets\n",
    "dim_date_hour_df = dates_df.crossJoin(hours).crossJoin(minute_buckets)\n",
    "\n",
    "# Add Date_Key\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# IsRushHour: 7-9am and 16-18pm\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"IsRushHour\",\n",
    "    when(\n",
    "        ((col(\"Hour\").between(7, 9)) | (col(\"Hour\").between(16, 18))),\n",
    "        lit(True)\n",
    "    ).otherwise(lit(False))\n",
    ")\n",
    "\n",
    "# Timeofday: Night (0-5), Morning (6-11), Afternoon (12-17), Evening (18-23)\n",
    "dim_date_hour_df = dim_date_hour_df.withColumn(\n",
    "    \"Timeofday\",\n",
    "    when(col(\"Hour\").between(0, 5), \"Night\")\n",
    "    .when(col(\"Hour\").between(6, 11), \"Morning\")\n",
    "    .when(col(\"Hour\").between(12, 17), \"Afternoon\")\n",
    "    .otherwise(\"Evening\")\n",
    ")\n",
    "\n",
    "# Select and reorder columns\n",
    "dim_date_hour_df = dim_date_hour_df.select(\n",
    "    \"Date_Key\", \"Hour\", \"MinuteBucket\", \"IsRushHour\", \"Timeofday\"\n",
    ")\n",
    "\n",
    "# Create managed table\n",
    "dim_date_hour_df.write.mode(\"overwrite\").saveAsTable(\"divvy.default.DIM_date_hour\")\n",
    "\n",
    "display(dim_date_hour_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5444801716230496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Draft",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
