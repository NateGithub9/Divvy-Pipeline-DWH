{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b326d492-4614-42a8-8987-20f6060f71d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#unzip \n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "ZIP_SOURCE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips\"\n",
    "UNZIPPED_DEST_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/raw\"\n",
    "ARCHIVE_PATH = \"abfss://divvycontainer@divvystorage1.dfs.core.windows.net/trips_archive\"\n",
    "\n",
    "dbutils.fs.mkdirs(ARCHIVE_PATH)\n",
    "\n",
    "def extract_and_write_zip_files():\n",
    "    zip_files = [\n",
    "        f.path for f in dbutils.fs.ls(ZIP_SOURCE_PATH)\n",
    "        if f.name.endswith('.zip')\n",
    "    ]\n",
    "    for zip_path in zip_files:\n",
    "        # Read ZIP file as binary\n",
    "        binary_df = spark.read.format(\"binaryFile\").load(zip_path)\n",
    "        zip_bytes = binary_df.collect()[0]['content']\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zip_ref:\n",
    "            for contained_file in zip_ref.namelist():\n",
    "                if contained_file.lower().endswith('.csv'):\n",
    "                    csv_bytes = zip_ref.read(contained_file)\n",
    "                    output_file_path = f\"{UNZIPPED_DEST_PATH}/{contained_file}\"\n",
    "                    dbutils.fs.put(\n",
    "                        output_file_path,\n",
    "                        csv_bytes.decode(\"utf-8\", errors='ignore'),\n",
    "                        overwrite=True\n",
    "                    )\n",
    "        # Move processed zip to archive\n",
    "        file_name = zip_path.split('/')[-1]\n",
    "        archive_target = f\"{ARCHIVE_PATH}/{file_name}\"\n",
    "        dbutils.fs.mv(zip_path, archive_target)\n",
    "\n",
    "extract_and_write_zip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f43e1edb-a177-48d6-896a-d76d29ddcc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trip column type\n",
    "df = spark.table(\"divvy.default.bronze_trip_data\")\n",
    "display(spark.createDataFrame(df.dtypes, [\"column\", \"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "501df412-f205-4583-b443-edf73be76b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#weather column type\n",
    "df = spark.table(\"divvy.default.bronze_weather_data\")\n",
    "display(spark.createDataFrame(df.dtypes, [\"column\", \"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ab62777c-5a16-420f-9118-8e74846a3ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DIM DATE DAY TABLE\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, date_format, year, quarter, month, dayofmonth, dayofweek, \n",
    "    when, lit, expr\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Add columns\n",
    "dim_date_df = dates_df \\\n",
    "    .withColumn(\"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month_Name\", date_format(col(\"Full_Date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"Day_of_Month\", dayofmonth(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_of_Week\", dayofweek(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_Name\", date_format(col(\"Full_Date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"Is_Weekend\", when(col(\"Day_of_Week\").isin([1,7]), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\n",
    "        \"Season\",\n",
    "        when(month(col(\"Full_Date\")).isin([12,1,2]), \"Winter\")\n",
    "        .when(month(col(\"Full_Date\")).isin([3,4,5]), \"Spring\")\n",
    "        .when(month(col(\"Full_Date\")).isin([6,7,8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    )\n",
    "\n",
    "# Reorder columns\n",
    "dim_date_df = dim_date_df.select(\n",
    "    \"Date_Key\", \"Full_Date\", \"Year\", \"Quarter\", \"Month\", \"Month_Name\",\n",
    "    \"Day_of_Month\", \"Day_of_Week\", \"Day_Name\", \"Is_Weekend\", \"Season\"\n",
    ")\n",
    "\n",
    "# Create managed table with correct schema and primary key\n",
    "dim_date_df.write \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"divvy.default.DIM_Date_day\")\n",
    "display(dim_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f8dad4b9-a805-4cec-a9a2-349855c7283e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DIM DATE DAY TABLE\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, date_format, year, quarter, month, dayofmonth, dayofweek, \n",
    "    when, lit, expr\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "  SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\n",
    "\"\"\").selectExpr(\"explode(date_seq) as Full_Date\")\n",
    "\n",
    "# Add columns\n",
    "dim_date_df = dates_df \\\n",
    "    .withColumn(\"Date_Key\", date_format(col(\"Full_Date\"), \"yyyyMMdd\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Year\", year(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Month_Name\", date_format(col(\"Full_Date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"Day_of_Month\", dayofmonth(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_of_Week\", dayofweek(col(\"Full_Date\"))) \\\n",
    "    .withColumn(\"Day_Name\", date_format(col(\"Full_Date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"Is_Weekend\", when(col(\"Day_of_Week\").isin([1,7]), lit(True)).otherwise(lit(False))) \\\n",
    "    .withColumn(\n",
    "        \"Season\",\n",
    "        when(month(col(\"Full_Date\")).isin([12,1,2]), \"Winter\")\n",
    "        .when(month(col(\"Full_Date\")).isin([3,4,5]), \"Spring\")\n",
    "        .when(month(col(\"Full_Date\")).isin([6,7,8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    )\n",
    "\n",
    "# Reorder columns\n",
    "dim_date_df = dim_date_df.select(\n",
    "    \"Date_Key\", \"Full_Date\", \"Year\", \"Quarter\", \"Month\", \"Month_Name\",\n",
    "    \"Day_of_Month\", \"Day_of_Week\", \"Day_Name\", \"Is_Weekend\", \"Season\"\n",
    ")\n",
    "\n",
    "# Create managed table\n",
    "dim_date_df.write.mode(\"overwrite\").saveAsTable(\"divvy.default.DIM_Date_day\")\n",
    "\n",
    "display(dim_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "90cc01a1-dfba-4b0b-b798-862fb820134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--first part of silver transformation (CTE inside A CTAS, distance and minutes calculations)\n",
    "CREATE TABLE silver_trip_data\n",
    "USING DELTA\n",
    "AS\n",
    "WITH T1 AS (\n",
    "    SELECT\n",
    "        ride_id,\n",
    "        rideable_type,\n",
    "        member_casual,\n",
    "        start_lat,\n",
    "        start_lng,\n",
    "        end_lat,\n",
    "        end_lng,\n",
    "        CAST(started_at AS TIMESTAMP) AS trip_start_ts,\n",
    "        CAST(ended_at AS TIMESTAMP) AS trip_end_ts,\n",
    "        COALESCE(start_station_name, 'UNKNOWN') AS start_station_name,\n",
    "        COALESCE(start_station_id, 'N/A') AS start_station_id,\n",
    "        COALESCE(end_station_name, 'UNKNOWN') AS end_station_name,\n",
    "        COALESCE(end_station_id, 'N/A') AS end_station_id\n",
    "    FROM\n",
    "        divvy.default.bronze_trip_data\n",
    "    WHERE \n",
    "        _rescued_data IS NULL\n",
    "        AND started_at IS NOT NULL\n",
    "        AND ended_at IS NOT NULL\n",
    "),\n",
    "\n",
    "Calculated AS (\n",
    "    SELECT\n",
    "        --Haversine Formula (The Haversine formula calculates the great-circle distance between two points on a sphere, given their latitudes and longitudes. It is commonly used in navigation and geospatial calculations.)\n",
    "        --(https://en.wikipedia.org/wiki/Haversine_formula)\n",
    "        *, \n",
    "        CASE\n",
    "            WHEN trip_end_ts > trip_start_ts\n",
    "            THEN ROUND(TIMESTAMPDIFF(SECOND, trip_start_ts, trip_end_ts) / 60.0, 2)\n",
    "            ELSE NULL\n",
    "        END AS Trip_Duration_Min,\n",
    "        ROUND(\n",
    "            6371 * 2 * ASIN(\n",
    "                SQRT(\n",
    "                    POW(SIN(RADIANS(end_lat - start_lat) / 2), 2) +\n",
    "                    COS(RADIANS(start_lat)) * COS(RADIANS(end_lat)) *\n",
    "                    POW(SIN(RADIANS(end_lng - start_lng) / 2), 2)\n",
    "                )\n",
    "            ),\n",
    "            2\n",
    "        ) AS Trip_Distance_Km,\n",
    "        CAST(trip_start_ts AS DATE) AS Full_Date\n",
    "    FROM\n",
    "        T1\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    T2.ride_id,\n",
    "    T2.rideable_type,\n",
    "    T2.member_casual,\n",
    "    T2.Full_Date,\n",
    "    T2.start_station_id,\n",
    "    T2.end_station_id,\n",
    "    T2.Trip_Duration_Min,\n",
    "    T2.Trip_Distance_Km,\n",
    "    T2.start_lat,\n",
    "    T2.start_lng,\n",
    "    T2.end_lat,\n",
    "    T2.end_lng,\n",
    "    CAST(NULL AS STRING) AS start_community_area,\n",
    "    CAST(NULL AS STRING) AS end_community_area\n",
    "FROM\n",
    "    Calculated AS T2\n",
    "WHERE\n",
    "    T2.Trip_Distance_Km > 0.05\n",
    "    AND T2.Trip_Duration_Min IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45c68af4-2d7d-4699-a61c-c7f4bee6721d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE silver_trip_data\n",
    "ADD CONSTRAINT silver_trip_data_pk PRIMARY KEY (ride_id);"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7215048650100018,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Draft",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
